
## 问题1:

1. 你的银行系统目前是单进程的还是多进程的？如果是多进程，如何保证数据的一致性和并发处理能力？

跟据这段提问学习： 在项目中单进程和多进程的区别！


搜寻资料：

https://www.cnblogs.com/darknebula/p/10072658.html

#### I/O多路复用：

---

操作系统基本概念：

 **用户空间 / 内核空间**：（证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。）

**进程切换**： （内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行）进程切换是非常耗费资源


**进程阻塞**： （正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态）

在计算机中常见的一些什么情况下会造成进程堵塞：

**文件描述符**： 当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。

**缓存I/O**： 缓存I/O又称为标准I/O ， （数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。）

---

#### 5种IO模型、阻塞IO和非阻塞IO、同步IO和异步IO

参考文章： https://cloud.tencent.com/developer/article/1684951

#### **阻塞 I/O（Blocking I/O）**

- **工作原理**：在阻塞 I/O 模式下，应用程序发起 I/O 请求时，程序会等待该请求完成，才会继续执行后续操作。在等待期间，程序无法做其他事情，必须等 I/O 操作完成后才能继续执行。
    
- **例子**：假设程序需要从文件中读取数据。当程序发起读取请求时，它会等待直到文件读取完成，才能继续处理后面的任务。

#### **非阻塞 I/O（Non-blocking I/O）**

- **工作原理**：在非阻塞 I/O 模式下，应用程序发起 I/O 请求时，不会等待请求完成，而是立即返回控制权。应用程序可以继续执行其他任务，如果 I/O 操作尚未完成，程序可以稍后再尝试读取数据，或者轮询检查 I/O 状态。
    
- **例子**：程序请求读取文件，如果文件没有数据，程序不会被阻塞，而是立即返回，可以继续做其他事情。稍后程序可以再次尝试读取文件，直到文件中有数据。

#### **同步 I/O（Synchronous I/O）**

- **工作原理**：在同步 I/O 模式下，应用程序会发起一个 I/O 请求并等待结果返回。即使在 I/O 操作执行期间，程序也不会继续执行其他任务，直到该请求完成。虽然程序本身是同步的，但可以是阻塞的或非阻塞的。
    
- **例子**：假设程序在从文件读取数据时，必须等待操作完成才能处理后续逻辑。

#### **异步 I/O（Asynchronous I/O）**

- **工作原理**：在异步 I/O 模式下，应用程序发起 I/O 请求后，不会等待请求完成，程序会继续执行其他任务。当 I/O 操作完成时，操作系统会通知应用程序，程序可以进行后续处理。应用程序的执行与 I/O 操作解耦。
    
- **例子**：程序请求读取文件时，操作系统异步地处理 I/O 请求。程序在等待的同时可以执行其他任务。当文件读取完成时，操作系统会通知程序处理数据。


#### 同步I/O和阻塞I/O的区别在哪里？

|特性 \ 模型|阻塞 I/O|非阻塞 I/O|同步 I/O|异步 I/O|
|---|---|---|---|---|

|   |   |   |   |   |
|---|---|---|---|---|
|**工作方式**|程序等待 I/O 操作完成|程序立即返回，稍后处理 I/O|程序等待 I/O 完成|程序立即返回，稍后处理 I/O|

|   |   |   |   |   |
|---|---|---|---|---|
|**程序行为**|阻塞，等待数据|继续执行其他任务|等待数据，阻塞执行|继续执行其他任务，通知处理数据|

|   |   |   |   |   |
|---|---|---|---|---|
|**适用场景**|简单、少量请求的处理|高效的 I/O 轮询|短时间 I/O 操作|高并发、长时间 I/O 操作|



对于一个网络输入操作通常包括两个不同阶段：

- 等待网络数据到达网卡→读取到内核缓冲区，数据准备好；
- 从内核缓冲区复制数据到进程空间。

进程空间：保证系统中每个任务或进程的安全;（ 保证系统中每个任务或进程的安全;） 



多个的进程的IO可以注册到一个复用器（select）上，然后用一个进程调用该select， select会监听所有注册进来的IO；

---

#### **多路复用 I/O（I/O Multiplexing）**

- **工作原理**：多路复用 I/O 模型允许单个进程或线程同时处理多个 I/O 操作。应用程序通过 `select`、`poll` 等机制等待多个文件描述符的状态变化（例如数据是否可读）。一旦某个文件描述符准备好进行 I/O 操作，程序就能处理该文件的 I/O 操作。
    
- **例子**：一个 Web 服务器需要同时处理多个用户的请求。通过多路复用模型，服务器可以同时监视多个连接，一旦某个连接的数据准备好，服务器就可以继续进行处理，而不是单独为每个连接分配一个线程。




### 同步阻塞（BIO）

- 服务端采用单线程，当 `accept` 一个请求后，在 `recv` 或 `send` 调用阻塞时，将无法 `accept` 其他请求（必须等上一个请求处理 `recv` 或 `send` 完 ）（无法处理并发）

- 服务端采用多线程，当 accept 一个请求后，开启线程进行 recv，可以完成并发处理，但随着请求数增加需要增加系统线程，大量的线程占用很大的内存空间，并且线程切换会带来很大的开销，10000个线程真正发生读写实际的线程数不会超过20%，每次accept都开一个线程也是一种资源浪费。
	

### 同步非阻塞（NIO）

- 服务器端当 `accept` 一个请求后，加入 `fds` 集合，每次轮询一遍 `fds` 集合 `recv` (非阻塞)数据，没有数据则立即返回错误，每次轮询所有 fd （包括没有发生读写实际的 fd）会很浪费 CPU。



#### 三种实现：

**select**

它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以**select具有O(n)的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

  


#### 数据库连接池

https://pkg.go.dev/github.com/jackc/pgx/v5/pgxpool

不使用数据库连接池的步骤：

1. TCP建立连接的三次握手
2. MySQL认证的三次握手
3. 真正的SQL执行
4. MySQL的关闭
5. TCP的四次握手关闭

可以看到，为了执行一条SQL，却多了非常多网络交互。

优点：

- 实现简单

缺点：

- 网络IO较多
- 数据库的负载较高
- 响应时间较长及QPS较低
- 应用频繁的创建连接和关闭连接，导致临时对象较多，GC频繁
- 在关闭连接后，会出现大量TIME_WAIT 的TCP状态（在2个MSL之后关闭）


#### 使用连接池

第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。

优点：

- 较少了网络开销
- 系统的性能会有一个实质的提升
- 没了麻烦的TIME_WAIT状态

#### 工作原理：

连接池的工作原理主要由三部分组成，分别为

- 连接池的建立
- 连接池中连接的使用管理
- 连接池的关闭


连接池管理策略是连接池机制的核心，连接池内连接的分配和释放对系统的性能有很大的影响。其管理策略是：

当客户请求数据库连接时，首先查看连接池中是否有空闲连接，如果存在空闲连接，则将连接分配给客户使用；如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。

当客户释放数据库连接时，先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。

该策略保证了数据库连接的有效复用，避免频繁的建立、释放连接所带来的系统资源开销。


#### 注意

1、并发问题

2、事务处理

3、连接池的分配与释放

４、连接池的配置与维护


为货币编写自定义校验器：能够被gin支持

validator.go

我的项目中已经使用了事务管理！

**db.go**

```go
// Code generated by sqlc. DO NOT EDIT.

// versions:

//   sqlc v1.25.0

  

package db

  

import (

    "context"

  

    "github.com/jackc/pgx/v5"

    "github.com/jackc/pgx/v5/pgconn"

)

  

type DBTX interface {

    Exec(context.Context, string, ...interface{}) (pgconn.CommandTag, error)

    Query(context.Context, string, ...interface{}) (pgx.Rows, error)

    QueryRow(context.Context, string, ...interface{}) pgx.Row

}

  

func New(db DBTX) *Queries {

    return &Queries{db: db}

}

  

type Queries struct {

    db DBTX

}

  

func (q *Queries) WithTx(tx pgx.Tx) *Queries {

    return &Queries{

        db: tx,

    }

}
```

**`DBTX` 接口**

- `DBTX` 是一个接口，定义了执行数据库操作的方法：
    - `Exec`：执行一个不返回行的 SQL 语句（例如 `INSERT`、`UPDATE`、`DELETE`）。
    - `Query`：执行一个返回多行的 SQL 查询。
    - `QueryRow`：执行一个返回单行的 SQL 查询。

**`Queries` 类型**

- `Queries` 是一个结构体，持有一个 `DBTX` 类型的字段 `db`，它是实际与数据库交互的对象。
- 通过 `New(db DBTX)` 可以创建一个 `Queries` 实例，`db` 可以是一个数据库连接对象或一个事务对象。

**`WithTx` 方法**

- `WithTx` 方法接受一个 `pgx.Tx` 类型的事务对象，并返回一个新的 `Queries` 实例，该实例将使用这个事务进行所有数据库操作。
- 这意味着在事务中执行的查询将通过事务进行处理，而不是直接与数据库连接交互。



**account_test.go**


```go


```

### **单元测试的基本框架**

- **`t *testing.T`**: 这是 Go 中标准的测试框架，用来管理测试的生命周期，记录失败情况。
- 
- **`testCases`**: 用于存储多个测试用例，每个测试用例包含一个名字 (`name`)、账户 ID (`accountID`)、测试设置 (`setupAuth`、`buildStubs`)、以及检查响应的函数 (`checkResponse`)。


### **模拟（Mock）和依赖注入（Dependency Injection）**

#### `gomock` 和 `mockdb`

- **`gomock`**: 这是 Go 的一个 mock 库，用于创建模拟对象。在测试中，我们不希望访问真实的数据库，因此使用 `gomock` 来模拟 `store` 对象的行为。
- **`mockdb.NewMockStore(ctrl)`**: 这是创建一个模拟的数据库对象，`ctrl` 是 `gomock.Controller`，它用于控制 mock 对象的生命周期。


#### 模拟数据库的行为

- **`buildStubs(store *mockdb.MockStore)`**: 这个字段是一个函数，用来定义模拟数据库的行为。例如，当调用 `store.GetAccount` 时返回预期的值。


gomock 打桩测试




**死锁：**


![[simplebank11.png]]

使用查询语句并在后面加上 FOR UPDATE 后 另一个事务无法查询到数据 必须等待第一个事务完成提交或回滚后第二个事务才可以返回数据


### **4. 什么情况下查询会阻塞？**

查询可能会阻塞的情况通常与锁有关，例如：

#### **共享锁（S锁）**

在某些隔离级别（如 **可串行化** 或 **SELECT...FOR UPDATE**）下，查询会加共享锁，其他事务可能会等待该锁释放后再操作。

#### **排他锁（X锁）**

当事务对数据行加排他锁（如执行 `UPDATE` 或 `DELETE` 操作）时，其他事务试图读取或修改数据可能会被阻塞。

#### **行为：**

- **查询行为**：读取 `id = 7` 的数据，同时对查询结果加排他锁（Exclusive Lock 或 X 锁）。
- **锁行为**：
    - 加锁的记录只能被当前事务修改，其他事务将被阻塞：
        - 其他事务无法对 `id = 7` 的记录进行 `UPDATE` 或 `DELETE` 操作。
        - 其他事务也无法再次对这条记录执行 `SELECT ... FOR UPDATE`。
    - 其他事务仍然可以对这条记录执行普通的 `SELECT` 查询。
- **并发性**：限制了其他事务对锁定记录的修改行为，适合需要确保数据一致性和避免竞争条件的场景。

#### **优点：**

- 防止数据被其他事务修改或删除，确保当前事务对数据具有独占性。
- 避免在事务后续的操作中，数据被其他事务更改。

#### **缺点：**

- 降低了并发性，可能会导致事务等待或死锁。


`FOR NO KEY UPDATE` 是一种锁模式，用于在查询中对特定的行加锁，但与 `FOR UPDATE` 的严格性稍微不同。它的作用是确保并发事务不会对被锁定的行执行可能导致主键或唯一键发生变化的操作。


当我们提交更新后的第一个数据之后 第二个事务将会得到更新后的数据



**ALTER TABLE "transfers" ADD FOREIGN KEY ("to_account_id") REFERENCES "accounts" ("id");**

这条语句：


**操作过程**

1. **获取锁**：
    
    - 对 `transfers` 表加 `SHARE ROW EXCLUSIVE` 锁。
    - 对 `accounts` 表加共享锁（如果需要验证数据）。
2. **检查现有数据**：
    
    - 数据库扫描 `transfers` 表中已有的 `to_account_id` 值，确保它们都存在于 `accounts` 表的 `id` 中。
    - 如果发现不一致的数据，操作会失败。
3. **更新元数据**：
    
    - 将新的外键约束写入系统表，确保约束从此生效。
4. **释放锁**：
    
    - 约束建立成功后，释放所有的锁。

---

**4. 外键与锁的影响**

 **4.1 并发影响**

- 在外键添加的过程中，表级锁可能会阻塞其他事务。例如：
    - 如果其他事务试图对 `transfers` 表执行 DDL 操作（如删除列、添加列等），会被阻塞。
    - 如果其他事务试图修改 `accounts` 表中相关的行（被外键引用的行），也会被阻塞。

**4.2 数据一致性保证**

- 外键约束的引入增加了数据一致性的保障，但也可能对性能产生一定影响，特别是在高并发的写入场景下。


**共享锁**
-  共享锁就是允许多个线程同时获取一个锁，一个锁可以同时被多个线程拥有。

account.sql 解决了因为转账发生的死锁问题！

#### **行级锁**

- `FOR NO KEY UPDATE` 是一种行级锁模式，属于 PostgreSQL 的锁机制，具体含义如下：
- 
- **用途**：
    - 锁住查询结果的行，防止其他事务对这些行的非键值列进行修改。
    - 对主键或唯一索引列（键值列）的插入或删除操作不受影响。
- **行为**：
    - 其他事务不能对已锁定的行执行 `UPDATE` 或 `DELETE` 操作。
    - 其他事务可以对未锁定的行进行读操作。
### **`FOR NO KEY UPDATE` 的作用**

`FOR NO KEY UPDATE` 锁模式允许事务锁定行，但与 `FOR UPDATE` 锁有所不同，具体表现为以下几点：

- **对主键或唯一索引列的修改不会被阻止**：`FOR NO KEY UPDATE` 锁不会阻止其他事务修改被锁定行的主键或唯一索引列。与 `FOR UPDATE` 锁不同，`FOR UPDATE` 会阻止对被锁定行的所有列的修改，包括主键列。


因此，当事务 1 和事务 2 尝试互相锁定对方的账户时，它们并不会因修改主键列而导致阻塞。**这样就避免了死锁，因为即使事务互相需要锁定对方的行，它们依然可以继续执行，不会互相阻塞**。


**死锁** 是指两个或多个事务在执行过程中，因互相等待对方释放资源而导致永远无法继续执行的情况。

```sql
BEGIN;
SELECT * FROM accounts WHERE id = 1 FOR UPDATE;  -- 锁定账户 A
UPDATE accounts SET balance = balance - 10 WHERE id = 1;
-- 接着尝试锁定账户 B
SELECT * FROM accounts WHERE id = 2 FOR UPDATE;
UPDATE accounts SET balance = balance + 10 WHERE id = 2;
COMMIT;

```





### 结构体设计问题：



为什么 RedisTaskProcessor 要定义这几个字段并且 函数最终为什么要返回这些字段：


```go
type TaskProcessor interface {

Start() error

Shutdown()

ProcessTaskSendVerifyEmail(ctx context.Context, task *asynq.Task) error

}

type RedisTaskProcessor struct {

server *asynq.Server

store db.Store

mailer mail.EmailSender

}

func NewRedisTaskProcessor(redisOpt asynq.RedisClientOpt, store db.Store, mailer mail.EmailSender) TaskProcessor {

logger := NewLogger()

redis.SetLogger(logger)

server := asynq.NewServer(

redisOpt,

asynq.Config{

Queues: map[string]int{

QueueCritical: 10,

QueueDefault: 5,

},

ErrorHandler: asynq.ErrorHandlerFunc(func(ctx context.Context, task *asynq.Task, err error) {

log.Error().Err(err).Str("type", task.Type()).

Bytes("payload", task.Payload()).Msg("process task failed")

}),

Logger: logger,

},

)

return &RedisTaskProcessor{

server: server,

store: store,

mailer: mailer,

}

} 
```



---

### 任务分发与处理：

反序列化和序列化在任务分发与处理中的作用不同，它们分别发生在任务的**生产**和**消费**阶段。

### 为什么需要反序列化？

在 `ProcessTaskSendVerifyEmail` 方法中，我们需要从队列中取出任务，并将任务的有效载荷（`payload`）反序列化成结构体 `PayloadSendVerifyEmail`，这样才能在任务处理中使用这些数据。

具体来说：

1. **反序列化** (`json.Unmarshal`) 将任务的有效载荷从 JSON 字符串格式转换回 Go 结构体，这样你就可以在代码中更方便地使用这些数据（例如 `payload.Username`, `payload.Email`）。
    
2. 你需要这些数据来执行后续的业务逻辑，比如查找数据库中的用户、生成验证码等。所以，你必须先将其转为一个结构体，才能在代码中像操作普通对象一样访问这些数据。
    

### 为什么需要序列化？

在 `DistributeTaskSendVerifyEmail` 方法中，我们将 `PayloadSendVerifyEmail` 结构体序列化成 JSON 格式，原因如下：

1. **数据传输**：任务队列中的任务实际上是在不同的系统或服务间传递的。为了使得任务能够被异步处理，我们将数据（即 `payload`）序列化为一个标准化格式（比如 JSON）。这样，无论任务队列在服务器之间如何传递，接收端都能够通过反序列化还原成原本的结构体。
    
2. **与队列系统的兼容**：任务队列（如 `asynq`）通常要求任务的数据必须是字节流（如 JSON 字符串）。因此，必须将结构体序列化成字节格式，才能通过队列传递。
    

 总结

- **序列化**：是将任务的 `Payload` 转换成 JSON 字符串，以便将其发送到队列中，进行任务的分发。
- **反序列化**：是从队列中取出任务后，将其有效载荷（JSON 格式）转换为 Go 结构体，便于在后端处理。

通过这种方式，你可以将任务内容从队列中抽象出来，然后在各个服务之间传递和处理，同时保持数据格式的标准化和一致性。

---

### 为什么不直接返回某个包的而是要封装成结构体后返回


```go
package worker

  

import (

    "context"

  

    "github.com/hibiken/asynq"

)

  

type TaskDistributor interface {

    DistributeTaskSendVerifyEmail(

        ctx context.Context,

        payload *PayloadSendVerifyEmail,

        opts ...asynq.Option,

    ) error

}

  

type RedisTaskDistributor struct {

    client *asynq.Client

}

  

func NewRedisTaskDistributor(redisOpt asynq.RedisClientOpt) TaskDistributor {

    client := asynq.NewClient(redisOpt)

    return &RedisTaskDistributor{

        client: client,

    }

}
```



在 `NewRedisTaskDistributor` 函数中返回 `client` 是为了在 `RedisTaskDistributor` 结构体中能够持有并使用 `asynq.Client`，从而让它能够执行任务队列相关的操作。

### 详细分析：

1. **初始化 `asynq.Client`**：
    
    ```go
    client := asynq.NewClient(redisOpt)
    ```
    
    `asynq.NewClient(redisOpt)` 创建了一个新的 `asynq.Client` 实例，这个实例用于与 Redis 进行交互。它负责将任务推送到 Redis 队列中，并提供任务相关的接口，例如 `EnqueueContext`，用于将任务入队。
    
2. **返回 `RedisTaskDistributor`**：
    
    ```go
    return &RedisTaskDistributor{
        client: client,
    }
    ```
    
    - `RedisTaskDistributor` 结构体包含一个 `client` 字段，这个字段会持有上面创建的 `asynq.Client` 实例。
    - 返回 `&RedisTaskDistributor{client: client}` 是为了将 `asynq.Client` 的实例注入到 `RedisTaskDistributor` 中。这样，`RedisTaskDistributor` 就可以通过 `client` 进行任务的分发操作。



**为什么要返回 `client`？**

并不是直接返回 `client`，而是将 `client` 放入 `RedisTaskDistributor` 结构体中。这样做的好处是：

- **封装任务分发逻辑**：将任务分发的功能（通过 `client`）封装到 `RedisTaskDistributor` 中，便于管理和维护。调用者不需要直接与 `asynq.Client` 打交道，只需要使用 `TaskDistributor` 接口中的方法。
- **依赖注入**：通过 `NewRedisTaskDistributor` 创建 `RedisTaskDistributor`，并把 `asynq.Client` 作为依赖传入，可以在不同的环境中灵活使用（例如：在测试时用不同的队列客户端替换 `asynq.Client`）。
- **松耦合**：通过接口 `TaskDistributor`，`RedisTaskDistributor` 可以更容易地替换为其他的任务分发实现（例如，使用 RabbitMQ 或 Kafka），而不需要修改外部调用者的代码。



**小结：**

- 返回的是 **`RedisTaskDistributor` 结构体**，而不是 `client`。
- `client` 被包含在 `RedisTaskDistributor` 中，用于处理任务分发的实际逻辑。
- 通过这种方式，你可以更好地组织代码和管理任务分发的依赖，提供灵活的替换和维护能力。



### 问题：

函数传参数的目的是什么没有的时候我看到复杂的项目，为什么要传入某些参数有点搞不明白，我局的应该是我对参数的理解不对

### 回答：

传递参数在函数中有着至关重要的作用，尤其是在处理复杂的项目时。参数的主要目的是为了让函数能够根据外部的不同需求执行特定的任务，同时保持函数的灵活性和可重用性。下面我会详细解释为什么要传递参数，以及它们的意义。


**提供输入：** 函数需要一些外部信息才能执行任务，而这些信息通常通过参数传递。例如，在你的代码中，`DistributeTaskSendVerifyEmail` 需要传递 `payload`，这代表着你要传递给任务的数据。没有这些参数，函数就无法完成它的工作。

**减少硬编码：** 如果在函数内部直接写死参数或数据，这样的函数就无法灵活应对不同的需求。例如，如果你在函数内部写死了某个任务的参数，那么你就只能执行固定的任务，这会大大限制代码的复用性。

**解耦合：** 通过函数参数，可以让函数和调用者之间的依赖关系更加清晰。调用者负责传递合适的数据，函数负责根据这些数据执行任务，从而减少函数与其他代码的紧耦合。

- **任务的输入数据：** 比如，你发送邮件的任务 `DistributeTaskSendVerifyEmail`，需要通过参数传递邮箱的内容、接收者等信息。
- **上下文数据：** 上下文信息（如 `context.Context`）通常用于处理超时、取消任务等。比如你在异步任务中，可能需要传递 `ctx` 来处理任务的生命周期。
- **配置选项：** 在某些情况下，函数的执行可能需要不同的配置选项，比如重试次数、队列名称等。这些都可以作为参数传递。

示例：

```go
func ProcessUserRegistration(ctx context.Context, username, email string) error { // 处理用户注册 }
```

你需要传递 `username` 和 `email`，因为这是处理用户注册所需要的基本信息。如果没有这些信息，函数就无法知道需要注册哪个用户。

